<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Leetcode 670. Maximum Swap]]></title>
    <url>%2F2019%2F10%2F14%2FLeetcode-670%2F</url>
    <content type="text"><![CDATA[https://leetcode.com/problems/maximum-swap/ num各位上的数字交换一次，使num达到能达到的最大值 12345678910111213141516class Solution &#123;public: int maximumSwap(int num) &#123; string str = to_string(num); int res = num; for(int i = 0; i &lt; str.size()-1; i++)&#123; for(int j = i + 1; j &lt; str.size(); j++)&#123; swap(str[i], str[j]); res = max(res, stoi(str)); swap(str[j], str[i]); &#125; &#125; return res; &#125;&#125;; 使用一个数组记录num从低位开始到该位所有数字的最大值，然后从num的高位开始遍历，如果高位上数字小于数组中记录的数字，就从低位查找到数组中记录的该数字的位置，然后进行交换返回。 123456789101112131415161718192021222324class Solution &#123;public: int maximumSwap(int num) &#123; string str = to_string(num); string arr = str; for(int i = arr.size()-2; i &gt;= 0; i--)&#123; arr[i] = max(arr[i], arr[i+1]); &#125; for(int i = 0; i &lt; str.size(); i++)&#123; if(str[i] &lt; arr[i])&#123; for(int j = str.size()-1; j &gt; i; j--)&#123; if(str[j] == arr[i])&#123; swap(str[i], str[j]); return stoi(str); &#125; &#125; &#125; &#125; return num; &#125;&#125;;]]></content>
      <categories>
        <category>Online-Judge</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 958 Check Completeness of a Binary Tree]]></title>
    <url>%2F2019%2F10%2F13%2FLeetcode-958%2F</url>
    <content type="text"><![CDATA[https://leetcode.com/problems/check-completeness-of-a-binary-tree/ 这道题比较简单，利用树的层序遍历，当第一次遇到空节点时，设置flag，如果第二次再遇到，那就返回false。 代码：123456789101112131415161718192021222324252627282930313233343536373839/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: bool isCompleteTree(TreeNode* root) &#123; if(!root)&#123; return true; &#125; queue&lt;TreeNode*&gt; q; q.push(root); int flag = 0; while(!q.empty())&#123; TreeNode * temp = q.front(); q.pop(); if(!temp)&#123; flag = 1; &#125; else&#123; if(flag)&#123; return false; &#125; q.push(temp-&gt;left); q.push(temp-&gt;right); &#125; &#125; return true; &#125;&#125;;]]></content>
      <categories>
        <category>Online Judge</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Leetcode 470 Implement Rand10() Using Rand7()]]></title>
    <url>%2F2019%2F10%2F13%2FLeetcode-470%2F</url>
    <content type="text"><![CDATA[https://leetcode.com/problems/implement-rand10-using-rand7/ 随机数的产生关键是等概率，这里有一个概念是拒绝采样，例如rand7()，等概率产生1～7的随机数，如果想要等概率得到1～5的数字，就不断rand7()，如果产生6,7就拒绝，如果是1～5范围内就接受。这样产生的数字在1到5范围内也是等概率的。 一些例子：123rand4()： (rand2() - 1) × 2 + rand2()rand2(): rand4() % 2 + 1 rand6() % 2 + 1 代码：12345678910111213// The rand7() API is already defined for you.// int rand7();// @return a random integer in the range 1 to 7class Solution &#123;public: int rand10() &#123; int val5, val6; while((val6 = rand7()) &gt; 6); while((val5 = rand7()) &gt; 5); return (val6 % 2) * 5 + val5; &#125;&#125;;]]></content>
      <categories>
        <category>Online Judge</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ODD papers reading]]></title>
    <url>%2F2019%2F09%2F18%2Fodd-papers-reading%2F</url>
    <content type="text"><![CDATA[Summary目前大部分论文都是用和原始模型训练集不同的数据集来作为out of distribution的数据集，。OOD——out of distributionODD——out of distribution detectionID——in distributionODD也称作： one-class classification, novelty detection, anomaly detection, outlier detection, selective prediction, open set recognition, Evaluation Metrics AUROC（Aera Under Receiver of Characters curve）：受试工作者特征曲线下的面积，ROC曲线的纵坐标是TPR (TP/TP+FN)，横坐标是FPR (FP/FP+TN) AUPR （Aera Under the Precision-Recall curve）：precision~recall FPR$N$：当有$N\%$的OOD样本被检测出来时，一个ID样本被检测错的概率，越低越好。 Deep anomaly detection with outlier exposure(ICLR’2019) Dan Hendrycks, Mantas Mazeika, Thomas G. Dietterich 主要内容： 在一些现有的ODD detectors上增加Outlier Exposure机制，设计了一个损失函数，帮助提高OOD检测的准确率$$\mathbb{E} _ {(x, y) \sim \mathcal{D} _ {in}} [ \mathcal{L} (f(x), y) + \lambda \mathbb{E} _ {x^{\prime} \sim \mathcal{D} _ {out}^{OE}} [ \mathcal{L}_{OE} (f(x^{\prime}), f(x), y) ] ]$$ 在baseline方法Maximum Softmax Probability、Confidence Branch和Density estimators增加了关于OOD样本的损失项，使得原有的detectors增强了检测OOD样本的能力。 代码实现（Pytorch）：https://github.com/hendrycks/outlier-exposure 在图片和文本数据集上进行实验 A baseline for detecting misclassified and out-of-distribution examples in neural networks(ICLR’17) Dan Hendrycks, Kevin Gimpel 主要内容： 利用softmax给出的信心值作为判断样本是否是OOD的依据 高置信度的错误预测经常是来自于softmax的，单从某个样本上看，softmax给的信心值可能是错的，但是从统计上看，错误分类的样本和OOD的样本在softmax给出的信心值上是比正常的正确分类的样本是要低的。 代码实现（Tensorflow）：https://github.com/hendrycks/error-detection Training confidence-calibrated classifiers for detecting out-of-distribution samples(ICLR’18) Kimin Lee, Honglak Lee, Kibok Lee, Jinwoo Shin 主要内容： 在原有模型的训练中，设计了一个新的损失函数（confidence loss）如下，如果样本是OOD的，预测输出的信心值趋近于一个常数值（如，0），如果样本是ID的，则是一般的预测信心值。$$\mathop{min}\limits_{\theta} \ \mathbb{E}_{P_{in} (\hat{x}, \hat{y}) } \ [-logP_{\theta}(y = \hat{y}|\hat{\mathbf{x}}) ] + \beta \mathbb{E}_{P_{out} (\mathbf{x})} \lbrack KL(\mathcal{U}(y) \Vert P_{\theta}(y|\mathbf{x})) \rbrack $$ 训练一个generator网络，用来生成OOD的样本，并且尽可能与ID样本是相近的$$\mathop{min}\limits_{G} \mathop{max}\limits_{D} \ \beta \mathbb{E}_{P_{G(\mathbf{x})}} [ KL (\mathcal(y) || P_{\theta}(y | \mathbf{x})) ] + \mathbb{E}_{P_{in(\mathbf{x})}} [ \mathop{log} D(\mathbf{x}) ] + \mathbb{E}_{P_{G(\mathbf{x})}} [ \mathop{log} (1 - D(\mathbf{x}) ) ]$$ 使用自定义的损失函数，交替训练原始网络和GAN至收敛。 代码实现（Pytorch）：https://github.com/alinlab/Confident_classifier Learning Confidence for Out-of-Distribution Detection in Neural Networks(‘18) Terrance DeVries, Graham W. Taylor 主要内容： 在网络的penultimate layer之后增加一个用来显示模型对样本分类信心程度大小的一个模块。 用单层或者多层的全连结神经网络来实现 设计的损失函数为$$\mathcal{L} = - \sum_{i=1}^{M} log (p_i^{\prime})y_i + - \lambda log (c)$$ 代码实现（Pytorch）：https://github.com/uoguelph-mlrg/confidence_estimation Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks(ICLR’18) Shiyu Liang, Yixuan Li, R. Srikant 主要内容： 在使用的softmax做为ID和OOD样本分类的基础上，增加了temp scaling和在input perturbing的机制。 Tempperature Scaling:$$S _ {i} (\mathbf{x}; T) = \frac{exp(f_i(\mathbf{x})/ T)}{\sum _ {j = 1} ^ {N} exp (f_j(\mathbf{x})/T)}$$ Input perturbing:$$\tilde{x} = x - \varepsilon sign (-\nabla _ {x} log S_{\tilde{y}} (\mathbf{x};T)) $$ 代码实现（Pytorch）：https://github.com/facebookresearch/odin Out-of-Distribution Detection using Multiple Semantic Label Representations(NeurIPS’18) Gabi Shalev, Yossi Adi, Joseph Keshet 主要内容： 在深度神经网络之后接入K个回归函数，输出K个不同词向量 用词向量之间的$L_2$范数距离做为衡量OOD样本的标准 代码实现： https://github.com/MLSpeech/semantic_OOD（代码仓库是空的。。。） A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks(NeurIPS’18) Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin 主要内容： 用于OOD样本检测和对抗样本检测 通过类条件高斯分布来表示每个类别的样本的特征的分布（这个有近似的理论依据），$f(\cdot)$为DNN的penultimate layer的输出。$$P(f(\mathbf{x}) | y = c) = \mathcal{N} (f(\mathbf{x}) | \mu _ {c}, \Sigma)$$ 通过计算待测样本和距离最近的类条件高斯分布之间的马式距离来作为ODD score。 通过在待测样本上增加扰动（在score增加的方向上）来使得ID样本和OOD样本区别更大 代码实现（Pytorch）：https://github.com/pokaxpoka/deep_Mahalanobis_detector How to know when machine learning does not know(cleverhans blog by Nicolas Papernot and Nicholas Frosst) 主要内容： 提取样本在神经网络中每一层的输出特征 在每一层的输出中，用k近邻找出与测试样本最相近的k个训练集中的样本 根据测试样本和找出训练样本的分类结果的差异来判断测试样本是否是OOD的 代码实现：https://github.com/rodgzilla/machine_learning_deep_knn Deep One-Class Classification(ICML’18) Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Emmanuel Müller, Marius Kloft 主要内容： 深度支持向量数据描述。。。将OOD样本的检测问题视为一个二分类问题，这方面研究还有很多工作，One-Class SVM（OC-SVM），编码解码器，GAN之类的。 目标是学习一个训练集数据的描述，用神经网络表示为一个超球，这个超球尽可能小地只包含ID样本，如果样本落在超球内，是ID样本，如果落在外面是，OOD样本。 代码实现（Pytorch）：https://github.com/lukasruff/Deep-SVDD 做实验时，使用数据集中的一类样本做为ID的，其他类样本为OOD的 能检测对抗样本 Deep autoencoding gaussian mixture model for unsupervised anomaly detection(ICLR’18) Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, Haifeng Chen 主要内容： 深度自动编码的高斯混合模型，无监督的异常检测问题 利用深度解码器来对输入进行降维 利用高斯混合模型通过density estimation来估计每个样本的是否是OOD的分数 代码实现（Pytorch）：https://github.com/danieltan07/dagmm （Tensorflow）：https://github.com/Newcomer520/tf-dagmm OODS datasets：http://odds.cs.stonybrook.edu/ Anomaly Detection with Generative Adversarial Networks(IPMI’17) Thomas Schlegl, Philipp Seeböck, Sebastian M. Waldstein, Ursula Schmidt-Erfurth, Georg Langs 主要内容： 通过GAN来学习 设计一种评价机制来给出样本一个分数做为判断是否是OOD的标准。 代码实现： Novelty Detection with GAN(‘18) Mark Kliger, Shachar Fleishman 主要内容： 使用原始训练集和其他分布的数据来训练generator 使得discriminator能够识别出OOD的样本 代码实现： Out-of-distribution detection using an ensemble of self supervised leave-out classifiers(ECCV’18) Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, Theodore L. Willke 主要内容： 使用集成的方法来判断样本是否是OOD的 每个分类器都是通过留出法来训练的 代码实现（Pytorch）：https://github.com/YU1ut/Ensemble-of-Leave-out-Classifiers Reducing Network Agnostophobia(NeurIPS’18) Akshay Raj Dhamija, Manuel Günther, Terrance E. Boult 主要内容： Agnostophobia：未知的恐惧 代码实现（Keras）：https://github.com/Vastlab/Reducing-Network-Agnostophobia Classification-Reconstruction Learning for Open-Set Recognition(‘18) Ryota Yoshihashi, Wen Shao, Rei Kawakami, Shaodi You, Makoto Iida, Takeshi Naemura 主要内容： - 代码实现： On the Foundations of Noise-free Selective ClassificationEl-Yaniv, Ran，YairWiener 主要内容： - 代码实现： Selective Classification for Deep Neural Networks(NeurIPS’17) Yonatan Geifman, Ran El-Yaniv 主要内容： 在原始网络的基础上再训练一个selective classifier 允许用户设置期望的风险水平 代码实现： To Trust Or Not To Trust A Classifier(NeurIPS’18) Heinrich Jiang, Been Kim, Melody Y. Guan, Maya Gupta 主要内容： 设计一种评分标准，分数高则认为分类器结果哦可信任，低则不可信任 分数为，测试样本到除了预测分类以外，其他分类中距离最近样本的距离和测试样本到预测分类样本的距离 代码实现： https://github.com/google/TrustScore Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles(NeurIPS’17) Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell 主要内容： - 代码实现（Tensorflow）：https://github.com/vvanirudh/deep-ensembles-uncertainty Open Category Detection with PAC Guarantees(ICML’18) Si Liu, Risheek Garrepalli, Thomas G. Dietterich, Alan Fern, Dan Hendrycks 代码实现（R）：https://github.com/liusi2019/ocd Metric Learning for Novelty and Anomaly Detection(BMVC’18) Marc Masana, Idoia Ruiz, Joan Serrat, Joost van de Weijer, Antonio M. Lopez 代码实现（Tensorflow）：https://mmasana.github.io/OoD_Mining/ WAIC, but Why? Generative Ensembles for Robust Anomaly Detection(‘18) Hyunsun Choi, Eric Jang, Alexander A. Alemi 主要内容： WAIC：估计训练集和测试集数据期望之间的差距 代码实现： Towards open world recognition(CVPR’15) Abhijit Bendale, Terrance Boult 主要内容： - 代码实现（MATLAB）：https://github.com/abhijitbendale/OWR Anomaly Detection using One-Class Neural Networks(‘18) Raghavendra Chalapathy, Aditya Krishna Menon, Sanjay Chawla 主要内容： - 代码实现（）： Figure of Merit Training for Detection and Spotting(NeurIPS’93) Eric I. Chang and Richard P. Lippmann 主要内容： - 代码实现： Towards Open Set Deep Networks(CVPR’16) Abhijit Bendale, Terrance Boult 主要内容： - 代码实现（caffe）： https://github.com/abhijitbendale/OSDN （keras）https://github.com/aadeshnpn/OSDN A loss framework for calibrated anomaly detection(NeurIPS’18) Aditya Krishna Menon, Robert C. Williamson 主要内容： - 代码实现（）：]]></content>
  </entry>
  <entry>
    <title><![CDATA[leetcode_137_Single_Number_II]]></title>
    <url>%2F2019%2F04%2F29%2Fleetcode-137-Single-Number-II%2F</url>
    <content type="text"><![CDATA[https://leetcode.com/problems/single-number-ii/ 感觉这道题好有意思。题目大意为：给一个数组，数组中除了一个数字出现一次外，其他数字均出现三次，找出这个出现一次的数字。 思路：从网上看的题解，利用位运算，记录整数的二进制表示中每一位1出现的次数。 代码：123456789101112class Solution &#123;public: int singleNumber(vector&lt;int&gt;&amp; nums) &#123; int ones = 0, twos = 0; for(int i = 0; i &lt; nums.size(); i++)&#123; ones = (nums[i] ^ ones) &amp; ~twos; twos = (nums[i] ^ twos) &amp; ~ones; &#125; return ones; &#125;&#125;;]]></content>
      <categories>
        <category>Online Judge</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu系统修复]]></title>
    <url>%2F2019%2F04%2F19%2FUbuntu%E7%B3%BB%E7%BB%9F%E4%BF%AE%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[早上在毫无心理准备和几个月没备份的情况下，Ubuntu进不去了，内心十分崩溃。 交代环境：Ubuntu16.04+Windows10双系统，NVIDIA 1080Ti显卡。 具体表现为，开机出现Grub引导界面，进入Ubuntu只有长时间黑屏，Ctrl+Alt+F1进入命令行也没有任何反应。可以进入Windows系统。 修复步骤 从Grub引导界面进入Ubuntu高级选项，选择recovery mode进入，会有一大堆的log不断输出，最后停止时会看到/dev/sdb4文件系统有错误，requires a manual fsck 但是我在之后recovery mode的命令行模式没有办法修复（我只尝试输入了fsck命令，没有指定/dev/sdb4，也许会有差别），这里不太确定。 从Ubuntu启动盘进入Try Ubuntu without install，这里可以看到之前Ubuntu系统中的文件，如果系统不能修复，应该可以从这里把文件和代码抢救回来了。 打开命令行，输入sudo fsck -f /dev/sdb4，我这里一定要加/dev/sdb4，否则没效果。执行完如果还有错误，重复执行上述命令直到没有错误为止。 重启电脑，从Grub正常进入Ubuntu，发现修复成功了，恢复正常了。 曲折之路 综合大部分CSDN的说法，应该是内核在你不知觉的时候升级了，而新的内核没有NVIDIA显卡驱动，所以会黑屏，尝试编辑Ubuntu内核命令行参数（在Grub界面上按e进入编辑），（ro改为rw，加上nomodeset等参数），因为的确发现Ubuntu高级选项中有好几个内核的版本，以为就是这个错误，但其实没有，应该我的内核和显卡驱动都是可以加载好的吧。 不明白为什么会有多个内核版本 不明白为什么我的电脑在跑稍大一点训练任务的时候就会重启，就是重启导致文件系统错误，才会今天开不了机。 总结 平时习惯照搬网上经验，还是要经过脑子思考才可以。]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络对抗样本和测试方向论文总结]]></title>
    <url>%2F2019%2F03%2F04%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC%E5%92%8C%E6%B5%8B%E8%AF%95%E6%96%B9%E5%90%91%E8%AE%BA%E6%96%87%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[把论文分为攻击、防御、测试、验证、其他几类。攻击指通过一定的方法设计，生成能够欺骗神经网络的对抗样本。防御指使神经网络能够抵抗对抗样本的攻击。测试感觉是软工领域的说法，一般目的是希望生成能够探测到更多神经网络错误的更完备的测试集。验证是从理论上分析神经网络的安全性，是最困难的领域。 攻击1.Intriguing properties of neural networks2014年的论文，第一次提出了对抗样本。即在原始图像中添加了人眼不易发觉的扰动，会造成神经网络分类错误，这是一种非常反直觉的现象，也揭示了神经网络在安全领域中的应用会存在很大的隐患。同时，本文提出了一种生成对抗样本的方法。 首先将生成对抗样本形式化为一个约束优化问题： \(Minimize \left \| r \right \|_2 subject to:\)\(1. f(x+r) = l\)\(2. x+r \in [0, 1]^m\) \( Minimize c|r| + loss_{f}(x+r, l) subject to x+r \in [0, 1]^m\)通过L-BFGS算法来进行求解。这篇论文中还提出了一种观点，神经网络的语义信息是存在于网络中高层单元空间中的，而不是在单独的个体中。因为通过实验发现单独的高层单元个体和随机线性组合的高层单元是没有什么区别的。同时，这篇论文也提出了对抗样本存在移植性，对抗训练等观点。 2.Explaining and Harnessing Adversarial Examples这篇论文中提出了FGSM算法，即Fast Gradient Sign Method，可以更加快速地生成对抗样本。生成扰动的具体公式为：$$\mathbf{\eta} = \epsilon sign(\nabla_{x}J(\mathbf{\theta} , \mathbf{x}, y))$$这里，$J$是神经网络的损失函数，$y$是输入样本的label，$\eta$是常数调节步长，$\theta$是神经网络模型的参数。 同时，这篇论文也提出了神经网络在对抗样本方面展现的脆弱性主要是因为神经网络的线性性和对抗训练的方法。这篇论文的FGSM是one step的，后续也提出了迭代的FGSM，可以用来求更近距离的对抗样本。 3.The Limitations of Deep Learning in Adversarial Settings这篇论文的提出的攻击方法是JSMA，即Jacobian Saliency Map Attack。首先解释什么是Jacobian Matrix：神经网络输出的每一维对输出的每一维求导。然后根据得到的Jacobian Matrix构造对抗显著图（Adversarial Saliency Maps）： 4.Towards Evaluating the Robustness of Neural NetworksCW攻击方法，因为对抗样本求解问题的难度问题，探索尝试新的目标函数和优化方法，并且给出了基于三种范数的攻击。 5.DeepFool: a simple and accurate method to fool deep neural networks利用直线距离最短的思想，设计优化方法。 6.Practical Black-Box Attacks against Machine Learning黑盒攻击 7.Universal adversarial perturbations8.One pixel attack for fooling deep neural networks防御1.Distillation as a Defense to Adversarial Perturbations against Deep Neural Networks2.Adversarial Sample Detection for Deep Neural Network through Model Mutation TestingICSE’19 3.Ensemble Adversarial Training: Attacks and DefensesICLR’18Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, Patrick McDaniel 测试1.DeepXplore: Automated Whitebox Testing of Deep Learning SystemsSOSP’17 Best Paper 2.DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous CarsICSE’18 3.DeepGauge: Multi-Granularity Testing Criteria for Deep Learning SystemsASE’18 Distinguished Paper 4.MODE: automated neural network model debugging via state differential analysis and input selectionFSE’18 5.Testing Deep Neural Networks6.DeepMutation: Mutation Testing of Deep Learning Systems7.Combinatorial Testing for Deep Learning Systems8.DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving SystemsASE’18 9.Guiding Deep Learning System Testing using Surprise AdequacyICSE’19 10.TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing验证1.Safety Verification of Deep Neural NetworksInvited paper at CAV’17Xiaowei Huang, Marta Kwiatkowska, Sen Wang, Min Wu 2.Reluplex: An Efficient SMT Solver for Verifying Deep Neural NetworksExtended version of a paper with the same title that appeared at CAV 2017Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer 其他1.Measuring Neural Net Robustness with ConstraintsNIPS’16 2.Evaluating the Robustness of Neural Networks: An Extreme Value Theory ApproachICLR’18]]></content>
      <categories>
        <category>论文阅读</category>
      </categories>
      <tags>
        <tag>神经网络</tag>
        <tag>对抗样本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客添加搜索功能]]></title>
    <url>%2F2018%2F08%2F16%2FHexo%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD%2F</url>
    <content type="text"><![CDATA[为自己的博客添加搜索功能。 local_search 安装hexo-generator-searchdb模块，在博客的根目录下执行以下命令： 1npm install hexo-generator-searchdb --save 在站点配置文件中加入如下内容： 12345search: path: search.xml field: post format: html limit: 10000 在主题配置文件中开启local_search功能： 12local_search: enable: true algolia_search官方教程]]></content>
      <categories>
        <category>博客</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ2251DungeonMaster]]></title>
    <url>%2F2018%2F08%2F15%2FPOJ2251DungeonMaster%2F</url>
    <content type="text"><![CDATA[kuangbin带你飞专题 题目POJ1321You are trapped in a 3D dungeon and need to find the quickest way out! The dungeon is composed of unit cubes which may or may not be filled with rock. It takes one minute to move one unit north, south, east, west, up or down. You cannot move diagonally and the maze is surrounded by solid rock on all sides. Is an escape possible? If yes, how long will it take? InputThe input consists of a number of dungeons. Each dungeon description starts with a line containing three integers L, R and C (all limited to 30 in size).L is the number of levels making up the dungeon.R and C are the number of rows and columns making up the plan of each level.Then there will follow L blocks of R lines each containing C characters. Each character describes one cell of the dungeon. A cell full of rock is indicated by a ‘#’ and empty cells are represented by a ‘.’. Your starting position is indicated by ‘S’ and the exit by the letter ‘E’. There’s a single blank line after each level. Input is terminated by three zeroes for L, R and C. OutputEach maze generates one line of output. If it is possible to reach the exit, print a line of the formEscaped in x minute(s). where x is replaced by the shortest time it takes to escape.If it is not possible to escape, print the lineTrapped! Sample Input123456789101112131415161718192021223 4 5S.....###..##..###.#############.####...###########.#######E1 3 3S###E####0 0 0 Sample Output12Escaped in 11 minute(s).Trapped! 思路题意大致是一个三层迷宫，从出发点S到终点E，求最少时间，考虑广搜求解最优解问题，使用队列保存每步的坐标，每次移动有6种情况可以用一个二维数组来表示，设一个visit数组来表示到每个位置的最少时间，每次都是以能够到达该位置的前一个位置的最少时间+1来更新。 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;cstdio&gt;//#include &lt;memory.h&gt;using namespace std;int n, k;char pan[10][10];int row[10];int col[10];int sum;int res;void dfs(int sum, int a)&#123; if(sum == k)&#123; res++; return; &#125; for(int i = a; i &lt; n; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; if(pan[i][j] == '#' &amp;&amp; !row[i] &amp;&amp; !col[j])&#123; row[i] = 1; col[j] = 1; dfs(sum+1, i+1); row[i] = 0; col[j] = 0; &#125; &#125; &#125;&#125;int main()&#123; while(scanf("%d %d", &amp;n, &amp;k) &amp;&amp; n != -1)&#123; //memset(row, 0, sizeof(row)); //memset(col, 0, sizeof(col)); for(int i = 0; i &lt; n; i++)&#123; row[i] = col[i] = 0; &#125; res = 0; for(int i = 0; i &lt; n; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; scanf(" %c", &amp;pan[i][j]); &#125; &#125; dfs(0, 0); printf("%d\n", res); &#125; return 0;&#125;]]></content>
      <categories>
        <category>kuangbin带你飞 专题一 简单搜索</category>
      </categories>
      <tags>
        <tag>acm</tag>
        <tag>bfs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[POJ1321棋盘问题]]></title>
    <url>%2F2018%2F08%2F15%2FPOJ1321%E6%A3%8B%E7%9B%98%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[kuangbin带你飞专题 题目POJ1321在一个给定形状的棋盘（形状可能是不规则的）上面摆放棋子，棋子没有区别。要求摆放时任意的两个棋子不能放在棋盘中的同一行或者同一列，请编程求解对于给定形状和大小的棋盘，摆放k个棋子的所有可行的摆放方案C。 Input输入含有多组测试数据。每组数据的第一行是两个正整数，n k，用一个空格隔开，表示了将在一个n*n的矩阵内描述棋盘，以及摆放棋子的数目。 n &lt;= 8 , k &lt;= n当为-1 -1时表示输入结束。随后的n行描述了棋盘的形状：每行有n个字符，其中 # 表示棋盘区域， . 表示空白区域（数据保证不出现多余的空白行或者空白列）。 Output对于每一组数据，给出一行输出，输出摆放的方案数目C （数据保证C&lt;2^31）。 Sample Input1234567892 1#..#4 4...#..#..#..#...-1 -1 Sample Output1221 思路深搜，回溯 代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;cstdio&gt;//#include &lt;memory.h&gt;using namespace std;int n, k;char pan[10][10];int row[10];int col[10];int sum;int res;void dfs(int sum, int a)&#123; if(sum == k)&#123; res++; return; &#125; for(int i = a; i &lt; n; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; if(pan[i][j] == '#' &amp;&amp; !row[i] &amp;&amp; !col[j])&#123; row[i] = 1; col[j] = 1; dfs(sum+1, i+1); row[i] = 0; col[j] = 0; &#125; &#125; &#125;&#125;int main()&#123; while(scanf("%d %d", &amp;n, &amp;k) &amp;&amp; n != -1)&#123; //memset(row, 0, sizeof(row)); //memset(col, 0, sizeof(col)); for(int i = 0; i &lt; n; i++)&#123; row[i] = col[i] = 0; &#125; res = 0; for(int i = 0; i &lt; n; i++)&#123; for(int j = 0; j &lt; n; j++)&#123; scanf(" %c", &amp;pan[i][j]); &#125; &#125; dfs(0, 0); printf("%d\n", res); &#125; return 0;&#125;]]></content>
      <categories>
        <category>kuangbin带你飞 专题一 简单搜索</category>
      </categories>
      <tags>
        <tag>acm</tag>
        <tag>dfs回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络的激活函数]]></title>
    <url>%2F2018%2F08%2F14%2F%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0-1%2F</url>
    <content type="text"><![CDATA[激活函数给神经网络引入了非线性因素，大大增加了神经网络的可解释性难度。常用激活函数大致经历了sigmoid到tanh到ReLU的演变过程。 sigmoid公式$$\sigma (x) = \frac{1}{1+e^{-x}}$$ 性质函数饱和导致梯度消失问题，当神经元的激活值在0和1附近时，梯度接近为0，会饱和，导致梯度消失，几乎没有信号通过神经网络传回上一层。函数输出不是以0为中心，导致梯度下降权重更新时出现z字型下降。 tanh双曲正切函数 公式$$\sigma (x) = \frac{e^x - e^{-x}}{e^x+e^{-x}}$$ 性质解决了sigmoid函数输出不以0为中心的问题，仍然存在饱和问题。 ReLURectified Linear Unit(ReLU)，目前最常用的激活函数。 公式$$\sigma (x) = max(x, 0)$$当输入x&gt;0时，输出为x，当输入x&lt;0时，输出为0。 性质计算简单，效率高，不会饱和，对随机梯度下降的收敛有巨大的加速作用。由于其稀疏激活性，ReLU神经元很容易die并且不可逆，导致数据多样化的丢失，模型无法学习到有效特征。 Leaky ReLU公式$$\sigma (x) = max(\varepsilon x, x)$$$\varepsilon$是一个比较小的负数梯度值，例如0.01。 性质使负轴信息不会全部丢失，解决了ReLU中神经元die掉的问题。 softmax将神经网络的输出转换为可能性大小，用在最后一层输出层，输出值在[0,1]范围内，同时各输出值相加和为1，满足输出值代表的可能性的概率分布。 公式$${\sigma (\mathbf{z})}_{j} = \frac{e^{z_j}}{\sum_{k=1}^{K}{e^{z_k}}},\mathrm{for}j=1,…,K$$ Referencehttps://zhuanlan.zhihu.com/p/32610035https://feisky.xyz/machine-learning/neural-networks/active.htmlhttps://www.jiqizhixin.com/articles/2017-11-02-26https://zh.wikipedia.org/wiki/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0https://www.jianshu.com/p/22d9720dbf1a]]></content>
      <categories>
        <category>Deep learning</category>
      </categories>
      <tags>
        <tag>neural network</tag>
        <tag>activation function</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[conda和pip换源]]></title>
    <url>%2F2018%2F08%2F09%2Fconda%E5%92%8Cpip%E6%8D%A2%E6%BA%90%2F</url>
    <content type="text"><![CDATA[推荐使用国内镜像下载 pip国内源清华大学：https://pypi.tuna.tsinghua.edu.cn/simple阿里云：http://mirrors.aliyun.com/pypi/simple/豆瓣：http://pypi.doubanio.com/simple/中科大：https://mirrors.ustc.edu.cn/pypi/web/simple/ 更换源以更换清华的源为例： 临时使用使用-i参数，加上下载地址示例：1pip install -i https://pypi.tuna.tsinghua.edu.cn/simple keras 永久使用Linux在~/.pip/pip.conf文件中加入如下内容，没有则创建该文件 Windows找到用户目录下的AppData文件夹（显示隐藏项目或者资源管理器中直接搜索%appdata%），进入Local创建pip文件夹，创建pip.ini文件，添加如下内容1234[global]timeout = 6000index-url = https://pypi.tuna.tsinghua.edu.cn/simpletrusted-host = pypi.tuna.tsinghua.edu.cn conda国内源conda国内的源貌似只有清华的：https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 更换源换源命令12conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes 换回默认源1conda config --remove-key channels]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>conda</tag>
        <tag>pip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cplex的使用环境]]></title>
    <url>%2F2018%2F08%2F09%2Fcplex%E7%9A%84%E4%BD%BF%E7%94%A8%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[最近的一个实验中用到了cplex和keras，现有的支持只有cplex的win版本，并且只支持python2的32位版本，所以在这里记录一下环境搭建过程。 CPLEXCPLEX是IBM发布的一款软件，提供了用来求解线性规划（LP）等相关问题的函数库，并且支持Python、Matlab等多种语言的调用，详细介绍可以参考IBM的官方介绍文档。 试过pip install cplex和conda install -c IBMDecisionOptimization docplex cplex这两种，import cplex没有问题，但是跑实验时会报错求解问题size超过限制。有安装可执行文件，所以双击正常安装，需要java环境，按照提示操作就可以，安装完成后的界面和eclipse相似。 Anacondapython选择通过anaconda来安装，anaconda包含了Python、Conda等一系列的工具包，使用起来很方便。官网下载安装包，正常安装 theano因为tensorflow在windows下不支持python2，所以用theano做keras的后端 安装MinGW试过CodeBlock内置的MinGW和单独下载安装MinGW，都不行，会报编译错误，只能通过conda来安装了，安装之前一定要换源1conda install mingw libpython 下载安装完成后，要在Path中添加环境变量D:(anaconda的安装目录)\MinGW\binD:(anaconda的安装目录)\MinGW\i686-w64-mingw32(依平台而定)\lib并且注意，Path中只能有这一个MniGw变量。 下载1pip install theano 添加PYTHONPATH环境变量D:(anaconda的安装目录)\Lib\site-packages\theano 配置文件在C:\Users(用户名)目录下新建.theanorc.txt文件，添加内容入下：12345[blas]ldflags=[gcc]cxxflags=-ID:\(anaconda的安装目录)\MinGW\i686-w64-mingw32\include keras安装之前一定要换源1pip install keras 因为keras默认后端是tensorflow，所以要修改位theano修改后端文件，在C:\Users(用户名).keras\keras.json中，把tensorflow改成theano python调用cplex在D:(cplex的安装目录)\cplex\python\x86_win32目录下运行setup.py文件1python setup.py install 生成build文件夹在环境变量PYTHONPATH中添加D:(cplex的安装目录)\cplex\python\x86_win32 这样应该就可以用了。]]></content>
      <categories>
        <category>环境搭建</category>
      </categories>
      <tags>
        <tag>anaconda</tag>
        <tag>theano</tag>
        <tag>keras</tag>
        <tag>cplex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python的几个内置函数]]></title>
    <url>%2F2018%2F01%2F29%2Fpython0%2F</url>
    <content type="text"><![CDATA[Python is beautiful. (本文python环境为2.7.12) lambda表达式语法1lambda (参数) : (计算表达式) 描述目前知道的，lambda表达式在c#、java8、python都支持，主要作用为定义了一个匿名函数，使代码更加简洁 map函数语法1map(function, iterable, ...) 参数 function: 函数 iterable: 一个或多个列表 python2返回列表 python3返回迭代器 描述map函数将列表元素依次传入函数，作为函数的参数值进行计算，返回计算结果，一般也为列表 示例12#python2.7print(map(lambda x, y : x+y, [1, 2, 3], [2, 3, 4])) #打印[3, 5, 7] reduce函数语法1reduce(function, iterable[, initializer]) 参数 function: 函数，有两个参数 iterable: 可迭代对象，例如，list initializer: 可选，初始参数 返回函数计算结果 描述若无初始参数，reduce函数将可迭代对象的第1、2个之传入函数中进行计算，之后将计算结果与第3个值传入函数中，依次递推 若有初始参数，则第一步将初始参数和可迭代对象中第1个值传入参数中 示例12345678def add(x, y): return x+yprint(reduce(add, [1, 2, 3])) #打印6print(reduce(lambda x, y: x+y, [1, 2, 3])) #打印6print(reduce(lambda x, y : x+y, [1, 2, 3], 100)) #打印106 zip函数语法1zip([iterable, ...]) 参数 iterable: 一个或多个迭代器 描述zip函数，可用于将迭代器中对应元素打包成元组，返回元组列表，若迭代器元素个数不一致时，以最少的元素个数为准。此外，zip函数传入*操作符，可将元组解压 示例12345678list1 = [1, 2, 3]list2 = [4, 5, 6]list3 = [7, 8]zipped = zip(list1, list2)print(zipped) #打印[(1, 4), (2, 5), (3, 6)]print(zip(list1, list3)) #打印[(1, 7), (2, 8)]print(zip(*zipped)) #打印[(1, 2, 3), (4, 5, 6)]]]></content>
      <categories>
        <category>编程语言</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
